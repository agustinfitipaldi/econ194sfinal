---
title: "The First Order Effects Are The Friends We Made Along The Way"
author: "Tony Segal, Agustin Fitipaldi, Andrew Reilly, Elijah Stevenson"
date: "June 5, 2025"
format: pdf
---

# Executive Summary

The objective of this paper is to show the journey our group went through in order to assess what effect the Silicon Valley Bank collapse had on the banking sector across states. In short, we found no statistically significant and causally probable effect.

## The Setup

In quarter 1 of 2023, SVB had an oopsie...

## Geographic Contagion

Our first thoughts went towards a simple geographic model of contagion. Perhaps California and its bordering states shared a special connection and when one of them suffered a banking crisis, they were all thrown off kilter while the non-western states remained fine.

To test this theory, we decided to split the states with the following group being the "treated" one: California, Oregon, Arizona, Washington, Idaho, Utah, Montana, Wyoming, Colorado, New Mexico. We then proceeded with a standard difference in difference model with state and year fixed effects. See the specification below,

$$
\text{CoreDeposits}_{it} = \beta_0 + \beta_1\text{Crash}_{it} + \gamma_i + \tau_t + \epsilon_{it}
$$ Where $\beta_0$ and $\beta_1$ are the intercept and slope coefficient parameters respectively. $\text{Crash}_{it}$ is a binary variable that is $1$ if state $i$ is in our treatment group after the Silicon Valley Bank crash. $\gamma_i$ represents state fixed effects and $\tau_t$ stands for time fixed effects.

Below you can see a standard graph showing averages in core deposits to total liabilities across both groups. You can see that movement is mostly similar, albeit with slightly off sync jumps, and the western states drop below the other states after the crash.

```{r}
#| warning: false
#| message: false
#| echo: false

# ========================== GLOBAL CONFIGURATION ==========================
# Edit these parameters to control the entire analysis

# Variable for analysis (both clustering and regression)
analysis_variable <- "Core Deposits to Total Liabilities - All Institutions"  # Main variable of interest
# Other options: "Equity Capital to Total Assets - All Institutions", 
#                "Net Loans & Leases to Total Assets - All Institutions",
#                "Cost of Funding Earning Assets - All Institutions", etc.

# States to include/exclude
include_california <- TRUE  # Include California in clustering and analysis
exclude_states <- c("Nevada")  # List of states to remove entirely
# Example: c("Nevada", "Delaware", "South Dakota") for multiple states

# Clustering configuration
include_california_in_clustering <- TRUE  # Include CA in the clustering algorithm itself
clustering_variable <- "Core Deposits to Total Liabilities - All Institutions"  # Variable used for k-means clustering
# Can be different from analysis_variable if desired

# Treatment timing
crash_date <- as.Date("2023-03-31")  # Silicon Valley Bank crash date
# Alternative dates for testing: "2020-03-31", "2021-03-31", "2022-03-31", "2024-03-31"

# Optimal matching parameters
min_group_size <- 15  # Minimum states per group in optimal matching
# Try different sizes: 5, 8, 10, 15, 20 to test sensitivity

#print("=== ANALYSIS CONFIGURATION ===")
#print(paste("Analysis variable:", analysis_variable))
#print(paste("Clustering variable:", clustering_variable))
#print(paste("Include California:", include_california))
#print(paste("Include CA in clustering:", #include_california_in_clustering))
#print(paste("Excluded states:", paste(exclude_states, #collapse = ", ")))
#print(paste("Crash date:", crash_date))
#print("===============================")

# ========================== DATA LOADING AND SETUP ==========================

library(dplyr)
library(readr)
library(gt)
library(ggplot2)
library(estimatr)
library(modelsummary)
library(tidyr)
library(cluster)

lbank <- read_csv("combined_fdic_data_log.csv")

#names(lbank)[10] <- "core_deposit"
lbank$State <- as.factor(lbank$State)
#lbank$core_deposit <- as.numeric(lbank$core_deposit)
lbank$Date <- as.Date(lbank$Date, format = "%m/%d/%Y")

# Apply global filtering
lbank <- lbank %>%
  filter(!State %in% exclude_states) %>%
  {if(!include_california) filter(., State != "California") else .}

# Ensure data types are correct after filtering
lbank$Date <- as.Date(lbank$Date)  # Ensure Date remains Date type after filtering
lbank$State <- droplevels(lbank$State)  # Remove unused factor levels from excluded states
```

```{r}
#| warning: false
#| message: false
#| echo: false

# Create geographic groupings (conditional on California inclusion)
if(include_california) {
  west_states <- c("California", "Oregon", "Arizona", "Washington", "Idaho", "Utah", "Montana", "Wyoming", "Colorado", "New Mexico")
} else {
  west_states <- c("Oregon", "Arizona", "Washington", "Idaho", "Utah", "Montana", "Wyoming", "Colorado", "New Mexico")
}

lbank$west <- ifelse(lbank$State %in% west_states, 1, 0)
lbank$year <- format(lbank$Date, "%Y")
lbank$post <- ifelse(lbank$Date >= crash_date, 1, 0)
lbank$post <- as.factor(lbank$post)
lbank$west <- as.factor(lbank$west)

# Calculate averages for Western and non-Western states using the analysis variable
avg_deposits <- lbank %>%
  group_by(Date, west) %>%
  summarize(avg_analysis_var = mean(get(analysis_variable), na.rm = TRUE)) %>%
  mutate(Region = ifelse(west == 1, "Western States", "Non-Western States"))

# Create single plot with averages
ggplot(data = avg_deposits,
       aes(x = Date, y = avg_analysis_var, color = Region)) +
  geom_line(size = 1.2) +
  geom_vline(xintercept = crash_date, color = "red", linetype = "dotted", linewidth = 1) +
  labs(title = paste("Geographic Analysis:", analysis_variable),
       subtitle = "Western vs Non-Western States",
       x = "Date",
       y = paste(analysis_variable, "(log)"),
       color = "Region") + 
  annotate("text", 
           x = crash_date, 
           y = min(avg_deposits$avg_analysis_var) + 0.01, 
           label = "Crash", 
           color = "red", 
           vjust = -0.5, 
           angle = 90, 
           size = 4) +
  theme_minimal()
```

In another chart, you can see a clear difference in difference graphed out. This show us (change in west)-(change in east), and you can see a bit of a jump in the second quarter of 2020, which lines up with our previously judged "lag" in the graph above. Our pretrends appear semi-passable, so we are good to look at regression results.

```{r}
#| warning: false
#| message: false
#| echo: false

# Calculate difference-in-differences: (change in western) - (change in non-western)
wide_data <- avg_deposits %>%
  ungroup() %>%  # This is the key - remove any grouping from earlier operations
  select(Date, Region, avg_analysis_var) %>%
  pivot_wider(names_from = Region, values_from = avg_analysis_var) %>%
  arrange(Date)

# Then calculate changes and difference-in-differences using lag
diff_data <- wide_data %>%
  mutate(
    Western_Change = `Western States` - lag(`Western States`),
    NonWestern_Change = `Non-Western States` - lag(`Non-Western States`),
    Difference_in_Differences = Western_Change - NonWestern_Change
  ) %>%
  filter(!is.na(Western_Change))  # Remove first row with NA

# Create differences-in-differences plot
ggplot(data = diff_data, aes(x = Date, y = Difference_in_Differences)) +
  geom_line(size = 1.2, color = "darkblue") +
  geom_hline(yintercept = 0, color = "black", linetype = "solid", alpha = 0.5) +
  geom_vline(xintercept = as.Date("2022-12-31"), color = "red", linetype = "dotted", linewidth = 1) +
  labs(title = paste("Geographic Diff-in-Diff:", analysis_variable),
       subtitle = paste("Change in", analysis_variable, ": (Western Change) - (Non-Western Change)"),
       x = "Date",
       y = "Difference-in-Differences (log)") +
  annotate("text", 
           x = crash_date, 
           y = max(diff_data$Difference_in_Differences) - 0.005, 
           label = "Crash", 
           color = "red", 
           vjust = -0.5, 
           angle = 90, 
           size = 4) +
  theme_minimal()
```

## Regression Results

Finally we reach our regression. With four models displaying the different combination of fixed effects. We see statistical significance almost across the board except for dropping out when quarter fixed effects are turned on and state fixed effects are turned off. So this feels like a good sign! The toughest model to pass, with both fixed effects turned on, yielded a -6.2% impact on the treated group when compared with the non-treated group and controlling for differences across state and across time.

```{r}
#| tbl-cap: "Effect of Silicon Valley Bank Crash on Western States"
#| label: tbl-geographic
#| echo: false

# Use global configuration for regression analysis
#print("=== GEOGRAPHIC REGRESSION ANALYSIS ===")
#print(paste("Dependent variable:", analysis_variable))
#print(paste("States included:", nrow(lbank %>% distinct(State))))

lbank$crash <- ifelse(lbank$west == 1 & lbank$post == 1, 1, 0)

# Create more appropriate time fixed effects for quarterly data
lbank$year <- as.numeric(format(lbank$Date, "%Y"))
lbank$quarter <- quarters(lbank$Date)
lbank$year_quarter <- paste(lbank$year, lbank$quarter, sep = "_")
lbank$year_quarter <- as.factor(lbank$year_quarter)
lbank$State <- as.factor(lbank$State)

#print("Treatment variable check:")
#print(paste("Crash observations:", sum(lbank$crash)))
#print(paste("Post-treatment periods:", length(unique(lbank$Date[lbank$post == 1]))))
#print(paste("Western states:", length(unique(lbank$State[lbank$west == 1]))))

# Create formula dynamically based on analysis_variable
# Use year-quarter fixed effects instead of date fixed effects for cleaner quarterly identification
base_formula <- as.formula(paste("`", analysis_variable, "` ~ crash", sep=""))
state_formula <- as.formula(paste("`", analysis_variable, "` ~ crash + State", sep=""))
time_formula <- as.formula(paste("`", analysis_variable, "` ~ crash + year_quarter", sep=""))
full_formula <- as.formula(paste("`", analysis_variable, "` ~ crash + State + year_quarter", sep=""))

#create 4 different models using the analysis variable
model1 <- lm_robust(base_formula, data = lbank, se_type = "HC1")
model2 <- lm_robust(state_formula, data = lbank, se_type = "HC1")
model3 <- lm_robust(time_formula, data = lbank, se_type = "HC1")
model4 <- lm_robust(full_formula, data = lbank, se_type = "HC1")

#create list of models
models_list <- list(
  "(1)" = model1,
  "(2)" = model2,
  "(3)" = model3,
  "(4)" = model4
)

modelsummary(models_list,
             title = paste("Effect of Silicon Valley Bank Crash on", analysis_variable),
             stars = TRUE,
             gof_map = c("nobs"),
             coef_map = c("crash" = "Bank Crash",
                          "(Intercept)" = "Intercept"),
             coef_omit = "State|year_quarter",
             fmt = "%.3f", 
             notes = "Robust standard errors in parenthesis.",
             add_rows = tribble(
               ~term,         ~"(1)",  ~"(2)",  ~"(3)",  ~"(4)",
               "State FE",    "No",    "Yes",   "No",    "Yes",
               "Quarter FE",  "No",    "No",    "Yes",   "Yes"
             ),
             output = "gt") %>% 
  tab_spanner(
    label = analysis_variable,
    columns = everything()
  ) %>% 
  tab_options(
    table.width = pct(90),
    heading.align = "left"
  )
```

However, in order to test robustness and probe at our theory, which had admitted holes from the beginning (what is it about physical borders that would affect this?) we tried different grouping mechanisms in search of more effect.

# Checking Our Work

The search began by looking down the road of K-Means Clustering. A tool from unsupervised machine learning which quantitatively finds clusters among data by calculating euclidean distances between points and minimizing it. However, we quickly discovered that you have to be very careful with this, as it treads the line on being a causal nightmare. If you just turn k-means clustering "on" and try and find your pre-trend groups that way, you'll get disastrous results, since, again, you'll be finding *clusters*, which is precisely what you're not supposed to find. You need two groups who behave *similarly* before the event, and then *diverge* after the event. In cluster terms, your clusters appear after the event, not before. But you cant go searching for clusters after the event either, because that's blatantly backwards and completely ignores the counter-factual requirement. Something else is needed!

## Optimal Matching

Let's state our problem in clearer terms: we need to find two groups such that they behave as close as possible *before* the event, and then behave as differently as possible *after* the event. Since we'll be talking about differences all the way, lets establish our problem space geometrically to aid intuition.

We'll begin by considering each state individually and calculating their *average changes in core deposits to total liabilities before the event* as well as *after*. Our result is two numbers for each state, we can put these numbers together and graph all these points on a "differential space", with our $x$ axis measuring average change before the event, and our $y$ axis measuring average change after the event. Then a state like California would have the coordinates $(1,2)$. Our problem now becomes simpler to see:

\<insert graph?\>

Our objective is to find two points, $(x_1,y_1)$ and $(x_2,y_2)$, our cluster means, such that the following conditions are met:

1.  Their $x$ coordinates are as close together as possible. Quantitatively, we're finding $\min(|x_1-x_2|)$. This ensures clean pre-trends.
2.  Their $y$ coordinates are as far apart as possible, $\max(|y_1-y_2|)$. This ensures great effect.
3.  A potential cluster point minimizes the distance between itself and the rest of the states. This one is more complicated to write down, but it involves calculating distances between points in $R^2$, so $\min(\sqrt{x_1+x_i+y_1+y_i})$, and likewise for the other point. This ensures our solution isn't the trivial one with both points at positive and negative infinity.

This apparently has a name already, its called Optimal Matching, and has its roots in genetics! Adapted for our econometrics problem, though, its a statistically-significant-diff-in-diff blood hound. Set loose on our data set, it will find the most optimal group according to the state data and crash date we supply to it.

```{r}
#| warning: false
#| message: false
#| echo: false

#print("=== OPTIMAL MATCHING APPROACH ===")

# Step 1: Calculate pre and post-treatment trends for each state
# Using global crash_date variable

state_trends <- lbank %>%
  group_by(State) %>%
  arrange(Date) %>%
  mutate(
    period_change = get(analysis_variable) - lag(get(analysis_variable)),
    is_pre = Date < crash_date,
    is_post = Date >= crash_date
  ) %>%
  filter(!is.na(period_change)) %>%
  summarize(
    pre_trend = mean(period_change[is_pre], na.rm = TRUE),
    post_trend = mean(period_change[is_post], na.rm = TRUE),
    pre_volatility = sd(period_change[is_pre], na.rm = TRUE),
    post_volatility = sd(period_change[is_post], na.rm = TRUE),
    n_pre_obs = sum(is_pre),
    n_post_obs = sum(is_post)
  ) %>%
  filter(n_pre_obs >= 2 & n_post_obs >= 2) %>%  # Ensure sufficient data
  ungroup()

#print(paste("States with sufficient data:", nrow(state_trends)))
#print("Summary of pre-treatment trends:")
#print(summary(state_trends$pre_trend))
#print("Summary of post-treatment trends:")
#print(summary(state_trends$post_trend))

# Step 2: Define the objective function
objective_function <- function(assignment, penalty_weight = 1.0) {
  if(length(unique(assignment)) != 2 || min(table(assignment)) < min_group_size) {
    return(-Inf)  # Ensure both groups have at least min_group_size states
  }
  
  group1_states <- state_trends$State[assignment == 1]
  group2_states <- state_trends$State[assignment == 2]
  
  # Calculate group means
  group1_pre <- mean(state_trends$pre_trend[assignment == 1])
  group2_pre <- mean(state_trends$pre_trend[assignment == 2])
  group1_post <- mean(state_trends$post_trend[assignment == 1])
  group2_post <- mean(state_trends$post_trend[assignment == 2])
  
  # Objective: maximize post-difference, minimize pre-difference
  pre_similarity <- -abs(group1_pre - group2_pre)  # Penalty for different pre-trends
  post_difference <- abs(group1_post - group2_post)  # Reward for different post-trends
  
  # Also add within-group cohesion (standard k-means objective)
  within_group_cohesion <- 0
  for(group in 1:2) {
    group_indices <- which(assignment == group)
    if(length(group_indices) > 1) {
      group_pre_var <- var(state_trends$pre_trend[group_indices])
      group_post_var <- var(state_trends$post_trend[group_indices])
      within_group_cohesion <- within_group_cohesion - (group_pre_var + group_post_var)
    }
  }
  
  total_objective <- post_difference + penalty_weight * pre_similarity + 0.1 * within_group_cohesion
  return(total_objective)
}

# Step 3: Optimization using multiple approaches
set.seed(123)

# Approach 1: Grid search on pre-treatment trend splits
pre_trend_median <- median(state_trends$pre_trend)
pre_trend_splits <- quantile(state_trends$pre_trend, probs = seq(0.2, 0.8, 0.1))

best_objective <- -Inf
best_assignment <- NULL
best_method <- ""

# Try median split
assignment_median <- ifelse(state_trends$pre_trend <= pre_trend_median, 1, 2)
obj_median <- objective_function(assignment_median)
if(obj_median > best_objective) {
  best_objective <- obj_median
  best_assignment <- assignment_median
  best_method <- "Median Split"
}

# Try different quantile splits
for(i in 1:length(pre_trend_splits)) {
  assignment_quant <- ifelse(state_trends$pre_trend <= pre_trend_splits[i], 1, 2)
  obj_quant <- objective_function(assignment_quant)
  if(obj_quant > best_objective) {
    best_objective <- obj_quant
    best_assignment <- assignment_quant
    best_method <- paste("Quantile", round(0.2 + 0.1*i, 1), "Split")
  }
}

# Approach 2: Random search for better solutions
for(iter in 1:100) {
  # Random assignment ensuring balanced groups
  n_states <- nrow(state_trends)
  group1_size <- sample(min_group_size:(n_states-min_group_size), 1)  # Ensure both groups have min_group_size+ states
  random_assignment <- rep(2, n_states)
  random_assignment[sample(n_states, group1_size)] <- 1
  
  obj_random <- objective_function(random_assignment)
  if(obj_random > best_objective) {
    best_objective <- obj_random
    best_assignment <- random_assignment
    best_method <- paste("Random Search Iter", iter)
  }
}

# Step 4: Analyze the best solution
state_trends$optimal_cluster <- best_assignment
state_trends$optimal_cluster_name <- paste("Optimal Group", best_assignment)

#print(paste("Best method:", best_method))
#print(paste("Best objective value:", round(best_objective, 4)))

# Group characteristics
optimal_summary <- state_trends %>%
  group_by(optimal_cluster_name) %>%
  summarize(
    n_states = n(),
    mean_pre_trend = round(mean(pre_trend), 4),
    mean_post_trend = round(mean(post_trend), 4),
    sd_pre_trend = round(sd(pre_trend), 4),
    sd_post_trend = round(sd(post_trend), 4)
  )

#print("Optimal grouping characteristics:")
#print(optimal_summary)

# Calculate the key metrics
group1_pre <- mean(state_trends$pre_trend[best_assignment == 1])
group2_pre <- mean(state_trends$pre_trend[best_assignment == 2])
group1_post <- mean(state_trends$post_trend[best_assignment == 1])
group2_post <- mean(state_trends$post_trend[best_assignment == 2])

#print(paste("Pre-treatment difference:", round(abs(group1_pre - group2_pre), 4)))
#print(paste("Post-treatment difference:", round(abs(group1_post - group2_post), 4)))

# Show state assignments
#print("State assignments:")
assignments_table <- state_trends %>%
  arrange(optimal_cluster, State) %>%
  select(State, optimal_cluster_name, pre_trend, post_trend)
#print(assignments_table)

# Create scatter plot of states in differential space
# Calculate cluster means for plotting
cluster_means <- state_trends %>%
  group_by(optimal_cluster) %>%
  summarize(
    mean_pre_trend = mean(pre_trend),
    mean_post_trend = mean(post_trend)
  ) %>%
  mutate(optimal_cluster_name = paste("Optimal Group", optimal_cluster))

# Define colors for the groups
group_colors <- c("Optimal Group 1" = "#E69F00", "Optimal Group 2" = "#56B4E9")

ggplot(state_trends, aes(x = pre_trend, y = post_trend, color = optimal_cluster_name)) +
  geom_point(size = 3, alpha = 0.7) +
  geom_point(data = cluster_means, aes(x = mean_pre_trend, y = mean_post_trend, color = optimal_cluster_name), 
             size = 6, shape = 17) +
  scale_color_manual(values = group_colors) +
  labs(title = "State Positions in Differential Space",
       subtitle = "Each point represents a state's average change before (x) and after (y) the SVB crash\nTriangles show optimal cluster means",
       x = "Average Change Before Event (Pre-Trend)",
       y = "Average Change After Event (Post-Trend)",
       color = "Group Assignment") +
  theme_minimal() +
  theme(plot.title = element_text(hjust = 0.5),
        plot.subtitle = element_text(hjust = 0.5)) +
  geom_hline(yintercept = 0, linetype = "dashed", alpha = 0.5) +
  geom_vline(xintercept = 0, linetype = "dashed", alpha = 0.5)

# Create comparison chart with geographic groupings
# Add geographic grouping to state_trends data
state_trends_geo <- state_trends %>%
  mutate(
    geographic_group = ifelse(State %in% west_states, "Western States", "Non-Western States")
  )

# Calculate geographic group means
geo_means <- state_trends_geo %>%
  group_by(geographic_group) %>%
  summarize(
    mean_pre_trend = mean(pre_trend),
    mean_post_trend = mean(post_trend)
  )

# Define colors for geographic groups
geo_colors <- c("Western States" = "#D55E00", "Non-Western States" = "#009E73")

ggplot(state_trends_geo, aes(x = pre_trend, y = post_trend, color = geographic_group)) +
  geom_point(size = 3, alpha = 0.7) +
  geom_point(data = geo_means, aes(x = mean_pre_trend, y = mean_post_trend, color = geographic_group), 
             size = 6, shape = 17) +
  scale_color_manual(values = geo_colors) +
  labs(title = "State Positions in Differential Space (Geographic Grouping)",
       subtitle = "Comparison showing geographic grouping vs optimal matching\nTriangles show geographic group means",
       x = "Average Change Before Event (Pre-Trend)",
       y = "Average Change After Event (Post-Trend)",
       color = "Geographic Group") +
  theme_minimal() +
  theme(plot.title = element_text(hjust = 0.5),
        plot.subtitle = element_text(hjust = 0.5)) +
  geom_hline(yintercept = 0, linetype = "dashed", alpha = 0.5) +
  geom_vline(xintercept = 0, linetype = "dashed", alpha = 0.5)
```

Here we can see the first fruits of our labor, a two line graph illustrating the pre-trend and post-trend moves of our new optimal groups.

```{r}
#| warning: false
#| message: false
#| echo: false

# Create analysis using optimal groups
lbank_optimal <- lbank %>%
  left_join(state_trends %>% select(State, optimal_cluster), by = "State") %>%
  filter(!is.na(optimal_cluster)) %>%
  mutate(optimal_cluster_name = paste("Optimal Group", optimal_cluster))

# Calculate averages for optimal groups
avg_deposits_optimal <- lbank_optimal %>%
  group_by(Date, optimal_cluster_name) %>%
  summarize(avg_analysis_var = mean(get(analysis_variable), na.rm = TRUE)) %>%
  ungroup()

# Create levels plot
ggplot(data = avg_deposits_optimal, aes(x = Date, y = avg_analysis_var, color = optimal_cluster_name)) +
  geom_line(size = 1.2) +
  geom_vline(xintercept = crash_date, color = "red", linetype = "dotted", linewidth = 1) +
  labs(title = paste("Optimal Matching Analysis:", analysis_variable),
       subtitle = "Groups Optimized for Parallel Pre-Trends & Maximal Post-Differences",
       x = "Date",
       y = paste(analysis_variable, "(log)"),
       color = "Group") + 
  annotate("text", 
           x = crash_date, 
           y = min(avg_deposits_optimal$avg_analysis_var) + 0.01, 
           label = "Crash", 
           color = "red", 
           vjust = -0.5, 
           angle = 90, 
           size = 4) +
  theme_minimal()
```

## 

It seems to have done its job extremely well, the lines are practically copies of eachother and there's a very clear separation after the crash. Let's take a peek at the diff-in-diff graph,

```{r}
#| warning: false
#| message: false
#| echo: false

# Create diff-in-diff plot
wide_data_optimal <- avg_deposits_optimal %>%
  pivot_wider(names_from = optimal_cluster_name, values_from = avg_analysis_var) %>%
  arrange(Date)

optimal_cols <- names(wide_data_optimal)[names(wide_data_optimal) != "Date"]
group1_name_opt <- optimal_cols[1]
group2_name_opt <- optimal_cols[2]

diff_data_optimal <- wide_data_optimal %>%
  mutate(
    Group1_Change = get(group1_name_opt) - lag(get(group1_name_opt)),
    Group2_Change = get(group2_name_opt) - lag(get(group2_name_opt)),
    Difference_in_Differences = Group1_Change - Group2_Change
  ) %>%
  filter(!is.na(Group1_Change))

ggplot(data = diff_data_optimal, aes(x = Date, y = Difference_in_Differences)) +
  geom_line(size = 1.2, color = "purple") +
  geom_hline(yintercept = 0, color = "black", linetype = "solid", alpha = 0.5) +
  geom_vline(xintercept = crash_date, color = "red", linetype = "dotted", linewidth = 1) +
  labs(title = paste("Optimal Matching Diff-in-Diff:", analysis_variable),
       subtitle = paste("Optimized Groups:", group1_name_opt, "vs", group2_name_opt),
       x = "Date",
       y = "Difference-in-Differences (log)") +
  annotate("text", 
           x = crash_date, 
           y = max(diff_data_optimal$Difference_in_Differences) - 0.08, 
           label = "Crash", 
           color = "red", 
           vjust = -0.5, 
           angle = 90, 
           size = 4) +
  theme_minimal()
```

Hmm... For comparisons sake its been placed on the same y-axis as the other separation, so you can see that there is very little chatter before the crash, as intended. But the effect also looks rather downsized, curious. Let's look at some regression numbers,

```{r}
#| tbl-cap: "Effect of Silicon Valley Bank Crash using Optimal Matching"
#| label: tbl-optimal
#| echo: false

# Create optimal matching treatment variable
lbank_reg_optimal <- lbank %>%
  left_join(state_trends %>% select(State, optimal_cluster), by = "State") %>%
  filter(!is.na(optimal_cluster)) %>%
  mutate(
    optimal_treatment = ifelse(optimal_cluster == 1, 1, 0),
    optimal_crash = ifelse(optimal_treatment == 1 & post == 1, 1, 0)
  )

# Create time fixed effects for optimal regression too
lbank_reg_optimal$year <- as.numeric(format(lbank_reg_optimal$Date, "%Y"))
lbank_reg_optimal$quarter <- quarters(lbank_reg_optimal$Date)
lbank_reg_optimal$year_quarter <- paste(lbank_reg_optimal$year, lbank_reg_optimal$quarter, sep = "_")
lbank_reg_optimal$year_quarter <- as.factor(lbank_reg_optimal$year_quarter)
lbank_reg_optimal$State <- as.factor(lbank_reg_optimal$State)

#print("=== OPTIMAL MATCHING REGRESSION SETUP ===")
#print(paste("Optimal Group 1 states:", sum(lbank_reg_optimal$optimal_treatment == 1 & !duplicated(lbank_reg_optimal$State))))
#print(paste("Optimal Group 2 states:", sum(lbank_reg_optimal$optimal_treatment == 0 & !duplicated(lbank_reg_optimal$State))))

# Create formulas for optimal matching approach (using same global analysis_variable)
base_formula_optimal <- as.formula(paste("`", analysis_variable, "` ~ optimal_crash", sep=""))
state_formula_optimal <- as.formula(paste("`", analysis_variable, "` ~ optimal_crash + State", sep=""))
time_formula_optimal <- as.formula(paste("`", analysis_variable, "` ~ optimal_crash + year_quarter", sep=""))
full_formula_optimal <- as.formula(paste("`", analysis_variable, "` ~ optimal_crash + State + year_quarter", sep=""))

# Create optimal matching models
model1_optimal <- lm_robust(base_formula_optimal, data = lbank_reg_optimal, se_type = "HC1")
model2_optimal <- lm_robust(state_formula_optimal, data = lbank_reg_optimal, se_type = "HC1")
model3_optimal <- lm_robust(time_formula_optimal, data = lbank_reg_optimal, se_type = "HC1")
model4_optimal <- lm_robust(full_formula_optimal, data = lbank_reg_optimal, se_type = "HC1")

# Create list of optimal matching models
models_list_optimal <- list(
  "(1)" = model1_optimal,
  "(2)" = model2_optimal,
  "(3)" = model3_optimal,
  "(4)" = model4_optimal
)

modelsummary(models_list_optimal,
             title = paste("Effect of Silicon Valley Bank Crash on", analysis_variable, "- Optimal Matching"),
             stars = TRUE,
             gof_map = c("nobs"),
             coef_map = c("optimal_crash" = "Optimal Groups Crash",
                          "(Intercept)" = "Intercept"),
             coef_omit = "State|year_quarter",
             fmt = "%.3f", 
             notes = "Robust standard errors in parenthesis. Groups optimized for parallel pre-trends.",
             add_rows = tribble(
               ~term,         ~"(1)",  ~"(2)",  ~"(3)",  ~"(4)",
               "State FE",    "No",    "Yes",   "No",    "Yes",
               "Quarter FE",  "No",    "No",    "Yes",   "Yes"
             ),
             output = "gt") %>% 
  tab_spanner(
    label = paste(analysis_variable, "- Optimal Matching"),
    columns = everything()
  ) %>% 
  tab_options(
    table.width = pct(90),
    heading.align = "left"
  )
```

Neat! Smaller effect, but much cleaner pre-trend, so rather convincing robustness check eh? The issue with this sentiment begins, however, with the fact that in this optimal matching, California falls in the *non-treated* group. That is to say, we found a statistically significant effect of -1.8% on core deposits to total liabilities after the silicon valley bank crisis for the following states:

![](images/MapChart_Map.png)

The issues never end though, because no matter what size group you try, or whether you take california in or out, or even move the crash date around, there are always statistically significant results with seemingly odd and not very consistent groups of states. This is very clearly a nest of spurious correlations, and points rather squarely at the conclusion that the crash of Silicon Valley Bank, *when look at through this set of data, which is quarterly*, had no real effect on banking across states.
